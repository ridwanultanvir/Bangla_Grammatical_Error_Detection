model_name: bert_crf_4cls_token

# model_id: banglabert_large
# model_id: banglabert_large_norm
# model_id: banglabert_large_lr1e-5_wd_0.05
# model_id: banglabert
# model_id: banglabert_adamwpt_warmup0.1_lsmth0.1_th0.1_fp16
model_id: banglabert_adamwpt_warmup0.1_lsmth0.1_th0.1_norm

# with_ground: false # true
with_ground: true

args:
  # fp16: True
  per_device_train_batch_size: 32 # 4 # originally used 2
  per_device_eval_batch_size: 32 # 4 # originally used 2

results_dir: ./results/fixed/${model_name}/${model_id}
dataset:
  name: toxic_spans_crf_4cls_tokens
  # model_checkpoint_name: ${results_dir}/final_model
  model_checkpoint_name: ${results_dir}/ckpts/checkpoint-3000
  train_files:
    # train: ./data/vasha23_4cls/train_4cls.csv # ./data/tsd_train.csv
    train: ./data/vasha23_4cls/train_4cls_norm.csv
    # validation: ./data/vasha23_4cls/test_4cls.csv # ./data/tsd_trial.csv
    validation: ./data/vasha23_4cls/test_4cls_norm.csv
  eval_files:
    # test: ./data/test_tsd.csv # ./data/tsd_test.csv
    test: ./data/test_tsd_norm.csv # ./data/tsd_test.csv
  label_cls: true
  cls_threshold: 0.3 # can be tuned
  token_threshold: 0.0 # can be tuned
  tokenizer_params:
    truncation: true
    max_length: 384 # originally we used 200
    padding: max_length
    return_offsets_mapping: true
pretrained_args:
  pretrained_model_name_or_path: ${dataset.model_checkpoint_name}
  num_labels: 5
  lstm_hidden_size: 128
  lstm_layers: 1

save_dir: ${results_dir}/preds/
