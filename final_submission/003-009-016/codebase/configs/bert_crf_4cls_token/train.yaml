model_name: bert_crf_4cls_token
pretrained_args:
  pretrained_model_name_or_path: csebuetnlp/banglabert
  # pretrained_model_name_or_path: csebuetnlp/banglabert_large
  # pretrained_model_name_or_path:  xlm-roberta-base
  # pretrained_model_name_or_path: microsoft/deberta-v3-large
  # csebuetnlp/banglat5 # csebuetnlp/banglabert_large # bert-large-uncased
  num_labels: 5
  lstm_hidden_size: 128
  lstm_layers: 1

# model_id: banglabert
# model_id: banglabert_large
# model_id: banglabert_large_norm
# model_id: banglabert_large_lr1e-5_wd_0.05
model_id: banglabert_adamwpt_warmup0.1_lsmth0.1_th0.1_norm

results_dir: ./results/fixed/${model_name}/${model_id}

args:
  optim: adamw_torch
  warmup_ratio: 0.1
  label_smoothing_factor: 0.1
  # fp16: True

  output_dir: ${results_dir}/ckpts/
  evaluation_strategy: steps
  eval_steps: 500
  logging_steps: 500
  logging_first_step: true
  logging_dir: ${results_dir}/logs/
  save_steps: 500
  per_device_train_batch_size: 8 # 4 # originally used 2
  per_device_eval_batch_size: 8 # 4 # originally used 2
  learning_rate: 2e-5
  # num_train_epochs: 10
  num_train_epochs: 30
  weight_decay: 0.01
  # learning_rate: 1e-5
  
  # weight_decay: 0.05
  seed: 42

save_model_path: ${results_dir}/final_model/
