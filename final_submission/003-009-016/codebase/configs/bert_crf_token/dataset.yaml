name: toxic_spans_crf_tokens
# model_checkpoint_name: csebuetnlp/banglabert
# model_checkpoint_name: microsoft/deberta-v3-large
model_checkpoint_name: csebuetnlp/banglabert_large
# model_checkpoint_name:  xlm-roberta-base
# csebuetnlp/banglat5 # csebuetnlp/banglabert_large # bert-large-uncased
train_files:
  train: ./data/vasha23/train.csv # ./data/tsd_train.csv
  validation: ./data/vasha23/valid.csv # ./data/tsd_trial.csv
eval_files:
  test: ./data/vasha23/test_tsd.csv # ./data/tsd_test.csv
label_cls: true
cls_threshold: 0.3 # can be tuned
token_threshold: 0.0 # can be tuned
tokenizer_params:
  truncation: true
  max_length: 384 # originally we used 200
  padding: max_length
  return_offsets_mapping: true
