model_name: autotoken_4cls

# model_id: xlm_roberta_large
# model_id: bert-base-multilingual-uncased
# model_id: bloom-560m
# model_id: banglishbert
# model_id: mGPT
# model_id: banglabert_10ep
# model_id: debertav3large
# model_id: deberta-v3-base
# model_id: bert_large_25ep
# model_id: banglabert_large_full_fold1
# model_id: banglabert
# model_id: banglabert_large
# model_id: banglabert_large_lr_2e-6
# model_id: banglabert_large_wd0.9
# model_id: banglabert_large_wd2_warmup0.1
model_id: banglabert_large_warmup0.1_lsmth0.1
# model_id: banglabert_warmup0.1_lsmth0.1

results_dir: ./results/fixed/bert_token_4cls/${model_id}
dataset:
  batch_size: 32
  name: toxic_spans_tokens_4cls
  # model_checkpoint_name: ${results_dir}/final_model
  # model_checkpoint_name: ${results_dir}/ckpts/checkpoint-15000

  # model_checkpoint_name: ${results_dir}/ckpts/checkpoint-3500
  model_checkpoint_name: ${results_dir}/ckpts/checkpoint-8000
  # {'eval_loss': 0.20323185622692108, 'eval_Token-Wise F1': 0.9362618195686405, 'eval_Offset-Wise F1': 0.7452573315843102, 'eval_runtime': 39.2, 'eval_samples_per_second': 47.883, 'eval_steps_per_second': 5.995, 'epoch': 2.13}

  # model_checkpoint_name: ${results_dir}/ckpts/checkpoint-3500
  train_files:
    train: ./data/vasha23_4cls/train_4cls.csv #./data/tsd_train.csv
    validation: ./data/vasha23_4cls/test_4cls.csv # ./data/tsd_trial.csv
    # original_test: ./data/tsd_test_spans.csv
  eval_files:
    # test: ./data/test_tsd.csv # ./data/tsd_test.csv
    test: ../external_datasets/sazzed2019sentiment/processed/all_p8500_n3307.csv
  label_cls: true
  cls_threshold: 0.3 # can be tuned
  token_threshold: 0.0 # can be tuned
  tokenizer_params:
    truncation: true
    max_length: 384 # originally we used 200
    padding: max_length
    return_offsets_mapping: true
pretrained_args:
  pretrained_model_name_or_path: ${dataset.model_checkpoint_name}
with_ground: false
# with_ground: true
save_dir: ${results_dir}/preds/
